---
title: "Model validation"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
library(rms)
library(dplyr)
library(tidyr)

library(caret)
library(ResourceSelection)
```

This notebook validates the prognostic model generated from EDP dataset on the DRC data.

```{r message=FALSE, cache=F}
source('./mdl-evaluation.R')
```

Fist, loading model:

```{r}
mdl_folder = "model-age-ct-symp"
load(paste0(mdl_folder, "/model.RData"))
```

Second, loading data:

```{r}
test_data_file = 'drc-data.csv' 
test_data <- read.table(test_data_file, sep=",", header=TRUE, na.strings="\\N")
  
# Renaming variables so the model can be applied on the data
test_data %>% 
  rename(
    Death = S7.FinalStatus,
    PatientAge = AD.Age,
    CT = Lab.CtNP.D1,
    AnyBleeding = S2.Bleeding,
    Diarrhoea = S2.Diarrhea,
    Breathlessness = S2.DifficultyBreathing, 
    SwallowingProblems = S2.DifficultySwallowing
    ) -> test_data

test_data %>%
 select(Death, PatientAge, CT, AnyBleeding, Diarrhoea, Breathlessness, SwallowingProblems) -> test_data

test_data %>% drop_na() -> test_data

test_data
```

Generate calibration and ROC plots

```{r}
pred <- predict(model, test_data)
probs <- exp(pred) / (1 + exp(pred))
obs <- test_data$Death

data = data.frame(out = obs, prob = probs)
data$logodds = log(data$prob / (1-data$prob))
m1 <- glm(data$out ~ -1 + offset(data$logodds), family = binomial)
m1

data$m1_pred <- predict(m1, type = "response")

cal_plot(m1, data, "original", "m1_pred", model_label ='', dir = mdl_folder, fn = "cal_plot_validation.pdf")
res <- cal_coeff(data, "m1_pred")
res
saveToTXT(res, mdl_folder, "cal_plot_coefficients.txt")
```

The plot above shows that the model is not well calibrated. The test below quantifies that:

```{r}
hl <- hoslem.test(data$out, data$prob, g=10)
print(hl)
```

The test rejects the null hypothesis of predicted and observed probabilities being the same.

The ROC plot including the AUC with CIs:

```{r}
roc_plot(obs, probs, dir = mdl_folder, fn = "roc_plot_validation.pdf")
```

```{r}

# Calculate perfromace measures using ROCit:
# https://cran.r-project.org/web/packages/ROCit/index.html
# https://cran.r-project.org/web/packages/ROCit/vignettes/my-vignette.html

measure <- measureit(score = probs, class = obs,
                     measure = c("ACC", "SENS", "SPEC", "FSCR"))

names(measure)
plot(measure$ACC~measure$Cutoff, type = "l")
plot(measure$SENS~measure$Cutoff, type = "l")
plot(measure$SPEC~measure$Cutoff, type = "l")
plot(measure$FSCR~measure$Cutoff, type = "l")
```

Finally, the confusion matrix is calculated below:

```{r}
conf <- confusionMatrix(data=as.factor(as.integer(probs > 0.63)), reference = as.factor(obs), positive = "1")
conf
saveToTXT(conf, mdl_folder, "conf_matrix.txt")
``` 